{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-19T10:03:52.060790Z",
     "start_time": "2025-03-19T10:03:46.642055Z"
    }
   },
   "source": [
    "import nltk\n",
    "import string\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import mlflow\n",
    "from sklearnex import patch_sklearn\n",
    "from warnings import filterwarnings\n",
    "patch_sklearn()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/uxlfoundation/scikit-learn-intelex)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T10:03:52.083153Z",
     "start_time": "2025-03-19T10:03:52.078778Z"
    }
   },
   "cell_type": "code",
   "source": "filterwarnings(\"ignore\")",
   "id": "1c721c30c36593c8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T10:06:28.641663Z",
     "start_time": "2025-03-19T10:06:19.454707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ],
   "id": "a3cbdf79a881bddc",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T11:14:40.444848Z",
     "start_time": "2025-03-19T11:14:40.441552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#text_preprocess_types = [None, '—Å—Ç–µ–º–º–∏–Ω–≥', '–ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è']\n",
    "text_preprocess_types = [None]\n",
    "#words_classes = ['N', 'NJ', 'NJV', 'ALL']\n",
    "words_classes = ['ALL']\n",
    "\n",
    "#frequency_filtration_types = [None, 'low', 'high', 'both']\n",
    "frequency_filtration_types = [None, 'low'] # 'high', 'both']\n",
    "\n",
    "vector_representation_types = ['binary', 'count', 'tfidf']"
   ],
   "id": "e43dd05f283d82ff",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T11:14:43.712378Z",
     "start_time": "2025-03-19T11:14:43.708378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "iterations_num = len(text_preprocess_types) * len(words_classes) * len(frequency_filtration_types)\n",
    "print(iterations_num)"
   ],
   "id": "5061d583c06cdce6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T10:11:12.860433Z",
     "start_time": "2025-03-19T10:11:12.855430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def base_ag_news_preprocess(text):\n",
    "    tokens = text.lower()\n",
    "\n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü —Å–ª–æ–≤\n",
    "    special_words = ['<br /><br />'] # –º–æ–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ä–∏–º—Å–∫–∏–µ —Ü–∏—Ñ—Ä—ã, —É–¥–∞–ª–∏—Ç—å film, movie, ...\n",
    "    for word in special_words:\n",
    "        tokens = tokens.replace(word, '')\n",
    "    \n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ —Ü–∏—Ñ—Ä\n",
    "    #tokens = ''.join(i if i not in set(string.punctuation) - set('-') | set(string.digits) else ' ' for i in tokens)\n",
    "    tokens = ''.join(i if i not in set(string.punctuation)  | set(string.digits) else ' ' for i in tokens)\n",
    "    \n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "    tokens = nltk.word_tokenize(tokens)\n",
    "    \n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø —Å–ª–æ–≤\n",
    "    stop_wordsL = stop_words - {'no','not'}\n",
    "    #stop_wordsL = stop_words\n",
    "    tokens = [word for word in tokens if (word not in stop_wordsL and word != '-')]\n",
    "    return tokens"
   ],
   "id": "c4de3f20c57a02f9",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T07:10:33.287797Z",
     "start_time": "2025-03-30T07:10:33.275702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def different_ag_news_preprocess(tokens, preprocess_type, words_class):\n",
    "    \n",
    "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª–æ–≤\n",
    "    if preprocess_type == '–ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è':\n",
    "        lemmatizer = nltk.WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    elif preprocess_type == '—Å—Ç–µ–º–º–∏–Ω–≥':\n",
    "        stemmer = nltk.PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏\n",
    "    tokens = pos_tag(tokens)\n",
    "    if words_class == 'N':\n",
    "        tokens = [word for word, tag in tokens if tag.startswith('N')]\n",
    "    elif words_class == 'NJ':\n",
    "        tokens = [word for word, tag in tokens if tag.startswith('N') or tag.startswith('J')]\n",
    "    elif words_class == 'NJV':\n",
    "        tokens = [word for word, tag in tokens if tag.startswith('N') or tag.startswith('J') or tag.startswith('V')]\n",
    "    elif words_class == 'ALL':\n",
    "        tokens = [word for word, _ in tokens]\n",
    "    \n",
    "    return tokens  "
   ],
   "id": "4b933f0767b60bc5",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T10:11:13.639317Z",
     "start_time": "2025-03-19T10:11:13.635812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def frequency_filtration(words_dictionary, frequency_filtration_type):\n",
    "    if frequency_filtration_type == 'low':\n",
    "        return dict([(key, value) for key, value in words_dictionary.items() if value >= 10 ])\n",
    "    elif frequency_filtration_type == 'high':\n",
    "        return dict([(key, value) for key, value in words_dictionary.items() if value <= 3000])\n",
    "    elif frequency_filtration_type == 'both':\n",
    "        return dict([(key, value) for key, value in words_dictionary.items() if 10 <= value <= 3000])\n",
    "    else:\n",
    "        return words_dictionary"
   ],
   "id": "46d4699c8beec784",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T10:11:13.950645Z",
     "start_time": "2025-03-19T10:11:13.947643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def dummy(doc):\n",
    "    return doc"
   ],
   "id": "9da65dd9e2d361a2",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T11:17:14.724173Z",
     "start_time": "2025-03-19T11:17:14.711454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def final_ag_news_preprocess(dataset, model_type):\n",
    "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    x_train = dataset['train']['text']\n",
    "    y_train = dataset['train']['label']\n",
    "    \n",
    "    x_test = dataset['test']['text']\n",
    "    y_test = dataset['test']['label']\n",
    "    \n",
    "    # –ë–∞–∑–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞\n",
    "    for i, text in enumerate(x_train):\n",
    "        x_train[i] = base_ag_news_preprocess(text)\n",
    "        \n",
    "    for i, text in enumerate(x_test):\n",
    "        x_test[i] = base_ag_news_preprocess(text)\n",
    "    \n",
    "    index = 0\n",
    "    # –í–∞—Ä–∏–∞—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞\n",
    "    for preprocess_type in text_preprocess_types: # 3 –≤–∞—Ä–∏–∞–Ω—Ç–∞\n",
    "        for words_class in words_classes:         # 4 –≤–∞—Ä–∏–∞–Ω—Ç–∞\n",
    "            words = {}\n",
    "            xtr = x_train\n",
    "            xte = x_test\n",
    "            \n",
    "            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "            for i, tokens in enumerate(xtr):\n",
    "                final_tokens = different_ag_news_preprocess(tokens, preprocess_type, words_class)\n",
    "                xtr[i] = final_tokens\n",
    "                \n",
    "                # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è\n",
    "                for token in final_tokens:\n",
    "                    if token not in words:\n",
    "                        words[token] = 1\n",
    "                    else:\n",
    "                        words[token] += 1\n",
    "            \n",
    "            xte = [different_ag_news_preprocess(tokens, preprocess_type, words_class) for tokens in xte]\n",
    "                    \n",
    "            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —á–∞—Å—Ç–æ—Ç–µ\n",
    "            for frequency_filtration_type in frequency_filtration_types: # 4 –≤–∞—Ä–∏–∞–Ω—Ç–∞\n",
    "                filtered_words = frequency_filtration(words, frequency_filtration_type)\n",
    "                token_length = len(filtered_words)\n",
    "                \n",
    "                # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Å–ª–æ–≤\n",
    "                word_list = sorted(filtered_words.keys())\n",
    "                # –ü—Ä–∏—Å–≤–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞–º –∏–Ω–¥–µ–∫—Å–æ–≤\n",
    "                words_indexed = {}\n",
    "                for idx, word in enumerate(word_list):\n",
    "                    words_indexed[word] = idx\n",
    "                \n",
    "                # OHE\n",
    "                vectorizer_OHE = CountVectorizer(vocabulary=words_indexed, tokenizer=dummy, preprocessor=dummy, dtype=np.int8, binary=True)\n",
    "                x_train_OHE = vectorizer_OHE.fit_transform(xtr)\n",
    "                x_test_OHE = vectorizer_OHE.transform(xte)\n",
    "                \n",
    "                # COUNT\n",
    "                vectorizer_COUNT = CountVectorizer(vocabulary=words_indexed, tokenizer=dummy, preprocessor=dummy, dtype=np.int8)\n",
    "                x_train_COUNT = vectorizer_COUNT.fit_transform(xtr)\n",
    "                x_test_COUNT = vectorizer_COUNT.transform(xte)\n",
    "                \n",
    "                # TF-IDF\n",
    "                vectorizer_TFIDF = TfidfVectorizer(vocabulary=words_indexed, preprocessor=dummy, tokenizer=dummy, dtype=np.float32)\n",
    "                x_train_TFIDF = vectorizer_TFIDF.fit_transform(xtr)\n",
    "                x_test_TFIDF = vectorizer_TFIDF.transform(xte)\n",
    "\n",
    "                # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤\n",
    "                if model_type == 'DT':\n",
    "                    clf_OHE = DecisionTreeClassifier()\n",
    "                    clf_COUNT = DecisionTreeClassifier()\n",
    "                    clf_TFIDF = DecisionTreeClassifier()\n",
    "                elif model_type == 'RF':\n",
    "                    clf_OHE = RandomForestClassifier(n_estimators=50, n_jobs=-1)\n",
    "                    clf_COUNT = RandomForestClassifier(n_estimators=50, n_jobs=-1)\n",
    "                    clf_TFIDF = RandomForestClassifier(n_estimators=50, n_jobs=-1)\n",
    "                elif model_type == 'ADA':\n",
    "                    clf_OHE =  AdaBoostClassifier(estimator=RandomForestClassifier(n_estimators=12, max_depth=75, n_jobs=-1), n_estimators=50, learning_rate=0.3)\n",
    "                    clf_COUNT =  AdaBoostClassifier(estimator=RandomForestClassifier(n_estimators=12, max_depth=75, n_jobs=-1), n_estimators=50, learning_rate=0.3)\n",
    "                    clf_TFIDF =  AdaBoostClassifier(estimator=RandomForestClassifier(n_estimators=12, max_depth=75, n_jobs=-1), n_estimators=50, learning_rate=0.3)\n",
    "                \n",
    "                # –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤\n",
    "                clf_OHE = clf_OHE.fit(x_train_OHE, y_train)\n",
    "                clf_COUNT = clf_COUNT.fit(x_train_COUNT, y_train)\n",
    "                clf_TFIDF = clf_TFIDF.fit(x_train_TFIDF, y_train)\n",
    "                \n",
    "                # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "                predictions_OHE = clf_OHE.predict(x_test_OHE)\n",
    "                predictions_COUNT = clf_COUNT.predict(x_test_COUNT)\n",
    "                predictions_TFIDF = clf_TFIDF.predict(x_test_TFIDF)\n",
    "                \n",
    "                # –ú–µ—Ç—Ä–∏–∫–∏\n",
    "                macro_score_OHE = f1_score(y_test, predictions_OHE)\n",
    "                macro_score_COUNT = f1_score(y_test, predictions_COUNT)\n",
    "                macro_score_TFIDF = f1_score(y_test, predictions_TFIDF)\n",
    "                \n",
    "                mlflow.start_run(run_name=f'{model_type}_{preprocess_type}_{words_class}_{frequency_filtration_type}_OHE_75')\n",
    "                mlflow.log_param('document vectorizer', 'OHE')\n",
    "                mlflow.log_param('model', clf_OHE.__class__.__name__)\n",
    "                mlflow.log_param('preprocess_type', preprocess_type)\n",
    "                mlflow.log_param('words_class', words_class)\n",
    "                mlflow.log_param('frequency_filtration_type', frequency_filtration_type)\n",
    "                mlflow.log_param('token_length', token_length)\n",
    "                #mlflow.sklearn.log_model(clf_OHE, 'DecisionTreeClassifier')\n",
    "                mlflow.log_metric('score', macro_score_OHE)\n",
    "                mlflow.end_run()\n",
    "                \n",
    "                mlflow.start_run(run_name=f'{model_type}_{preprocess_type}_{words_class}_{frequency_filtration_type}_COUNT_75')\n",
    "                mlflow.log_param('document vectorizer', 'COUNT')\n",
    "                mlflow.log_param('model', clf_COUNT.__class__.__name__)\n",
    "                mlflow.log_param('preprocess_type', preprocess_type)\n",
    "                mlflow.log_param('words_class', words_class)\n",
    "                mlflow.log_param('frequency_filtration_type', frequency_filtration_type)\n",
    "                mlflow.log_param('token_length', token_length)\n",
    "                #mlflow.sklearn.log_model(clf_COUNT, 'DecisionTreeClassifier')\n",
    "                mlflow.log_metric('score', macro_score_COUNT)\n",
    "                mlflow.end_run()\n",
    "                \n",
    "                mlflow.start_run(run_name=f'{model_type}_{preprocess_type}_{words_class}_{frequency_filtration_type}_TFIDF_75')\n",
    "                mlflow.log_param('document vectorizer', 'TFIDF')\n",
    "                mlflow.log_param('model', clf_TFIDF.__class__.__name__)\n",
    "                mlflow.log_param('preprocess_type', preprocess_type)\n",
    "                mlflow.log_param('words_class', words_class)\n",
    "                mlflow.log_param('frequency_filtration_type', frequency_filtration_type)\n",
    "                mlflow.log_param('token_length', token_length)\n",
    "                #mlflow.sklearn.log_model(clf_TFIDF, 'DecisionTreeClassifier')\n",
    "                mlflow.log_metric('score', macro_score_TFIDF)\n",
    "                mlflow.end_run()\n",
    "                \n",
    "                index += 1\n",
    "                print(f'–ò—Ç–µ—Ä–∞—Ü–∏—è {index} / {iterations_num}')"
   ],
   "id": "660b6dd207e48d59",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T10:11:16.175162Z",
     "start_time": "2025-03-19T10:11:16.166515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def one_run(dataset, preprocess_type, words_class, frequency_filtration_type, model_type):\n",
    "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    x_train = dataset['train']['text']\n",
    "    y_train = dataset['train']['label']\n",
    "    \n",
    "    x_test = dataset['test']['text']\n",
    "    y_test = dataset['test']['label']\n",
    "    \n",
    "    # –ë–∞–∑–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞\n",
    "    for i, text in enumerate(x_train):\n",
    "        x_train[i] = base_ag_news_preprocess(text)\n",
    "        \n",
    "    for i, text in enumerate(x_test):\n",
    "        x_test[i] = base_ag_news_preprocess(text)\n",
    "    \n",
    "    words = {}\n",
    "    xtr = x_train\n",
    "    xte = x_test\n",
    "            \n",
    "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "    for i, tokens in enumerate(xtr):\n",
    "        final_tokens = different_ag_news_preprocess(tokens, preprocess_type, words_class)\n",
    "        xtr[i] = final_tokens\n",
    "                \n",
    "        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è\n",
    "        for token in final_tokens:\n",
    "            if token not in words:\n",
    "                words[token] = 1\n",
    "            else:\n",
    "                words[token] += 1\n",
    "            \n",
    "    xte = [different_ag_news_preprocess(tokens, preprocess_type, words_class) for tokens in xte]\n",
    "                    \n",
    "    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —á–∞—Å—Ç–æ—Ç–µ\n",
    "    filtered_words = frequency_filtration(words, frequency_filtration_type)\n",
    "    token_length = len(filtered_words)\n",
    "                \n",
    "    # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Å–ª–æ–≤\n",
    "    word_list = sorted(filtered_words.keys())\n",
    "    # –ü—Ä–∏—Å–≤–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞–º –∏–Ω–¥–µ–∫—Å–æ–≤\n",
    "    words_indexed = {}\n",
    "    for idx, word in enumerate(word_list):\n",
    "        words_indexed[word] = idx\n",
    "        \n",
    "    # OHE\n",
    "    vectorizer_OHE = CountVectorizer(vocabulary=words_indexed, tokenizer=dummy, preprocessor=dummy, dtype=np.int8, binary=True)\n",
    "    x_train_OHE = vectorizer_OHE.fit_transform(xtr)\n",
    "    x_test_OHE = vectorizer_OHE.transform(xte)\n",
    "                \n",
    "    # COUNT\n",
    "    vectorizer_COUNT = CountVectorizer(vocabulary=words_indexed, tokenizer=dummy, preprocessor=dummy, dtype=np.int8)\n",
    "    x_train_COUNT = vectorizer_COUNT.fit_transform(xtr)\n",
    "    x_test_COUNT = vectorizer_COUNT.transform(xte)\n",
    "                \n",
    "    # TF-IDF\n",
    "    vectorizer_TFIDF = TfidfVectorizer(vocabulary=words_indexed, preprocessor=dummy, tokenizer=dummy, dtype=np.float32)\n",
    "    x_train_TFIDF = vectorizer_TFIDF.fit_transform(xtr)\n",
    "    x_test_TFIDF = vectorizer_TFIDF.transform(xte)\n",
    "    \n",
    "    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤\n",
    "    if model_type == 'DT':\n",
    "        clf_OHE = DecisionTreeClassifier()\n",
    "        clf_COUNT = DecisionTreeClassifier()\n",
    "        clf_TFIDF = DecisionTreeClassifier()\n",
    "    elif model_type == 'RF':\n",
    "        clf_OHE = RandomForestClassifier(n_estimators=50, n_jobs=-1)\n",
    "        clf_COUNT = RandomForestClassifier(n_estimators=50, n_jobs=-1)\n",
    "        clf_TFIDF = RandomForestClassifier(n_estimators=50, n_jobs=-1)\n",
    "    \n",
    "    # –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤\n",
    "    clf_OHE = clf_OHE.fit(x_train_OHE, y_train)\n",
    "    clf_COUNT = clf_COUNT.fit(x_train_COUNT, y_train)\n",
    "    clf_TFIDF = clf_TFIDF.fit(x_train_TFIDF, y_train)\n",
    "                \n",
    "    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "    predictions_OHE = clf_OHE.predict(x_test_OHE)\n",
    "    predictions_COUNT = clf_COUNT.predict(x_test_COUNT)\n",
    "    predictions_TFIDF = clf_TFIDF.predict(x_test_TFIDF)\n",
    "                \n",
    "    # –ú–µ—Ç—Ä–∏–∫–∏\n",
    "    macro_score_OHE = f1_score(y_test, predictions_OHE, average='macro')\n",
    "    macro_score_COUNT = f1_score(y_test, predictions_COUNT, average='macro')\n",
    "    macro_score_TFIDF = f1_score(y_test, predictions_TFIDF, average='macro')\n",
    "    \n",
    "    mlflow.start_run(run_name=f'test_{preprocess_type}_{words_class}_{frequency_filtration_type}_OHE')\n",
    "    mlflow.log_param('model', clf_OHE.__class__.__name__)\n",
    "    mlflow.log_param('tree_depth', clf_OHE.get_depth())\n",
    "    mlflow.log_param('preprocess_type', preprocess_type)\n",
    "    mlflow.log_param('words_class', words_class)\n",
    "    mlflow.log_param('frequency_filtration_type', frequency_filtration_type)\n",
    "    mlflow.log_param('token_length', token_length)\n",
    "    mlflow.sklearn.log_model(clf_OHE, 'DecisionTreeClassifier')\n",
    "    mlflow.log_metric('macro_score', macro_score_OHE)\n",
    "    mlflow.end_run()\n",
    "    \n",
    "    mlflow.start_run(run_name=f'test_{preprocess_type}_{words_class}_{frequency_filtration_type}_COUNT')\n",
    "    mlflow.log_param('model', clf_COUNT.__class__.__name__)\n",
    "    mlflow.log_param('tree_depth', clf_TFIDF.get_depth())\n",
    "    mlflow.log_param('preprocess_type', preprocess_type)\n",
    "    mlflow.log_param('words_class', words_class)\n",
    "    mlflow.log_param('frequency_filtration_type', frequency_filtration_type)\n",
    "    mlflow.log_param('token_length', token_length)\n",
    "    mlflow.sklearn.log_model(clf_COUNT, 'DecisionTreeClassifier')\n",
    "    mlflow.log_metric('macro_score', macro_score_COUNT)\n",
    "    mlflow.end_run()\n",
    "    \n",
    "    mlflow.start_run(run_name=f'test_{preprocess_type}_{words_class}_{frequency_filtration_type}_TFIDF')\n",
    "    mlflow.log_param('model', clf_TFIDF.__class__.__name__)\n",
    "    mlflow.log_param('tree_depth', clf_TFIDF.get_depth())\n",
    "    mlflow.log_param('preprocess_type', preprocess_type)\n",
    "    mlflow.log_param('words_class', words_class)\n",
    "    mlflow.log_param('frequency_filtration_type', frequency_filtration_type)\n",
    "    mlflow.log_param('token_length', token_length)\n",
    "    mlflow.sklearn.log_model(clf_TFIDF, 'DecisionTreeClassifier')\n",
    "    mlflow.log_metric('macro_score', macro_score_TFIDF)\n",
    "    mlflow.end_run()\n",
    "    "
   ],
   "id": "356187a9d739b4bd",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T10:09:25.782979Z",
     "start_time": "2025-03-19T10:09:25.679015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(experiment_name=\"IMDB(RF + ADA\")"
   ],
   "id": "3aa85a435033448",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/19 13:09:25 INFO mlflow.tracking.fluent: Experiment with name 'IMDB(RF + ADA' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/804641535566338505', creation_time=1742378965745, experiment_id='804641535566338505', last_update_time=1742378965745, lifecycle_stage='active', name='IMDB(RF + ADA', tags={}>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T11:26:03.738789Z",
     "start_time": "2025-03-19T11:17:19.206363Z"
    }
   },
   "cell_type": "code",
   "source": "final_ag_news_preprocess(imdb_dataset, model_type='ADA')",
   "id": "d127817ddf38f7b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run ADA_None_ALL_None_OHE_75 at: http://127.0.0.1:5000/#/experiments/804641535566338505/runs/9f06c7914d624732beb7cf76e56a2249\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/804641535566338505\n",
      "üèÉ View run ADA_None_ALL_None_COUNT_75 at: http://127.0.0.1:5000/#/experiments/804641535566338505/runs/000f14918e2e41ab92e6082f1a2cb912\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/804641535566338505\n",
      "üèÉ View run ADA_None_ALL_None_TFIDF_75 at: http://127.0.0.1:5000/#/experiments/804641535566338505/runs/04a841748515471cb7246159e5488ca2\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/804641535566338505\n",
      "–ò—Ç–µ—Ä–∞—Ü–∏—è 1 / 2\n",
      "üèÉ View run ADA_None_ALL_low_OHE_75 at: http://127.0.0.1:5000/#/experiments/804641535566338505/runs/08d1da72848349d082ecc5b357867180\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/804641535566338505\n",
      "üèÉ View run ADA_None_ALL_low_COUNT_75 at: http://127.0.0.1:5000/#/experiments/804641535566338505/runs/d5761051bbaa4882a42a12124bb00da3\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/804641535566338505\n",
      "üèÉ View run ADA_None_ALL_low_TFIDF_75 at: http://127.0.0.1:5000/#/experiments/804641535566338505/runs/7bc04a5afe98452bb145206039b8ba89\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/804641535566338505\n",
      "–ò—Ç–µ—Ä–∞—Ü–∏—è 2 / 2\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T10:10:42.773670Z",
     "start_time": "2025-03-19T10:10:42.770630Z"
    }
   },
   "cell_type": "code",
   "source": "#one_run(ag_news_dataset, '–ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è', 'NJ', 'low', 'RF')",
   "id": "7e2889f5c696e2af",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T20:52:13.948573Z",
     "start_time": "2025-03-10T20:52:13.945574Z"
    }
   },
   "cell_type": "code",
   "source": "cc = DecisionTreeClassifier()",
   "id": "488d299b7a4c3fb",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T19:48:36.328004Z",
     "start_time": "2025-03-11T19:46:01.378840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = imdb_dataset\n",
    "preprocess_type = '–ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è'\n",
    "words_class = 'NJ'\n",
    "frequency_filtration_type = 'low'\n",
    "\n",
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "x_train = dataset['train']['text']\n",
    "y_train = dataset['train']['label']\n",
    "    \n",
    "x_test = dataset['test']['text']\n",
    "y_test = dataset['test']['label']\n",
    "    \n",
    "# –ë–∞–∑–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞\n",
    "for i, text in enumerate(x_train):\n",
    "    x_train[i] = base_ag_news_preprocess(text)\n",
    "        \n",
    "for i, text in enumerate(x_test):\n",
    "    x_test[i] = base_ag_news_preprocess(text)\n",
    "    \n",
    "words = {}\n",
    "xtr = x_train\n",
    "xte = x_test\n",
    "            \n",
    "# –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "for i, tokens in enumerate(xtr):\n",
    "    final_tokens = different_ag_news_preprocess(tokens, preprocess_type, words_class)\n",
    "    xtr[i] = final_tokens\n",
    "                \n",
    "    # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è\n",
    "    for token in final_tokens:\n",
    "        if token not in words:\n",
    "            words[token] = 1\n",
    "        else:\n",
    "            words[token] += 1\n",
    "            \n",
    "xte = [different_ag_news_preprocess(tokens, preprocess_type, words_class) for tokens in xte]\n",
    "                    \n",
    "# –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —á–∞—Å—Ç–æ—Ç–µ\n",
    "filtered_words = frequency_filtration(words, frequency_filtration_type)\n",
    "token_length = len(filtered_words)\n",
    "                \n",
    "# –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Å–ª–æ–≤\n",
    "word_list = sorted(filtered_words.keys())\n",
    "# –ü—Ä–∏—Å–≤–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞–º –∏–Ω–¥–µ–∫—Å–æ–≤\n",
    "words_indexed = {}\n",
    "for idx, word in enumerate(word_list):\n",
    "    words_indexed[word] = idx\n",
    "    \n",
    "# OHE\n",
    "vectorizer_OHE = CountVectorizer(vocabulary=words_indexed, tokenizer=dummy, preprocessor=dummy, dtype=np.int8, binary=True)\n",
    "x_train_OHE = vectorizer_OHE.fit_transform(xtr)\n",
    "x_test_OHE = vectorizer_OHE.transform(xte)\n",
    "            \n",
    "# COUNT\n",
    "vectorizer_COUNT = CountVectorizer(vocabulary=words_indexed, tokenizer=dummy, preprocessor=dummy, dtype=np.int8)\n",
    "x_train_COUNT = vectorizer_COUNT.fit_transform(xtr)\n",
    "x_test_COUNT = vectorizer_COUNT.transform(xte)\n",
    "            \n",
    "# TF-IDF\n",
    "vectorizer_TFIDF = TfidfVectorizer(vocabulary=words_indexed, preprocessor=dummy, tokenizer=dummy, dtype=np.float32)\n",
    "x_train_TFIDF = vectorizer_TFIDF.fit_transform(xtr)\n",
    "x_test_TFIDF = vectorizer_TFIDF.transform(xte)"
   ],
   "id": "f8ebf9b51009ab3b",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T14:35:04.986578Z",
     "start_time": "2025-03-11T14:34:35.266864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clf_OHE = RandomForestClassifier(n_estimators=50, n_jobs=-1)\n",
    "clf_OHE = clf_OHE.fit(x_train_OHE, y_train)"
   ],
   "id": "ead27b1f895e7b07",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T14:35:07.931201Z",
     "start_time": "2025-03-11T14:35:07.876599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictions_OHE = clf_OHE.predict(x_test_OHE)\n",
    "macro_score_OHE = f1_score(y_test, predictions_OHE, average='macro')"
   ],
   "id": "9a32753c76fee33",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T14:35:09.132790Z",
     "start_time": "2025-03-11T14:35:09.129164Z"
    }
   },
   "cell_type": "code",
   "source": "print(macro_score_OHE)",
   "id": "7f1bb1652b7a6ad9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8811401647147361\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T15:20:18.000064Z",
     "start_time": "2025-03-11T15:20:17.996919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clf_COUNT = RandomForestClassifier(n_estimators=50, n_jobs=-1)\n",
    "#clf_COUNT = clf_COUNT.fit(x_train_COUNT, y_train)"
   ],
   "id": "615418eb24d06ed6",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T14:38:05.316804Z",
     "start_time": "2025-03-11T14:38:05.258759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictions_COUNT = clf_COUNT.predict(x_test_OHE)\n",
    "macro_score_COUNT = f1_score(y_test, predictions_COUNT, average='macro')"
   ],
   "id": "10f85fd99fae60c6",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T14:38:06.420221Z",
     "start_time": "2025-03-11T14:38:06.417415Z"
    }
   },
   "cell_type": "code",
   "source": "print(macro_score_COUNT)",
   "id": "f35908b8befb5d35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8793384460507578\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T14:39:15.599507Z",
     "start_time": "2025-03-11T14:38:48.295663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clf_TFIDF = RandomForestClassifier(n_estimators=50, n_jobs=-1)\n",
    "clf_TFIDF = clf_TFIDF.fit(x_train_TFIDF, y_train)"
   ],
   "id": "dae2fdb7ef004750",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T14:39:28.895743Z",
     "start_time": "2025-03-11T14:39:28.838718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictions_TFIDF = clf_TFIDF.predict(x_test_TFIDF)\n",
    "macro_score_TFIDF = f1_score(y_test, predictions_TFIDF, average='macro')"
   ],
   "id": "31f28f4f206c1fdc",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T14:45:21.724126Z",
     "start_time": "2025-03-11T14:45:21.721087Z"
    }
   },
   "cell_type": "code",
   "source": "print(macro_score_TFIDF)",
   "id": "f12b7a39ddc8ec94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8800442265813493\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T20:20:38.351570Z",
     "start_time": "2025-03-11T20:19:21.439227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clf_OHE_GB = GradientBoostingClassifier()\n",
    "clf_OHE_GB = clf_OHE_GB.fit(x_train_OHE, y_train)"
   ],
   "id": "d1d995f525bcc89a",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T20:20:40.907320Z",
     "start_time": "2025-03-11T20:20:40.877164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictions_GB_OHE = clf_OHE_GB.predict(x_test_OHE)\n",
    "macro_score_OHE_GB = f1_score(y_test, predictions_GB_OHE, average='macro')"
   ],
   "id": "8472e8a82bbb934d",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T20:20:42.226203Z",
     "start_time": "2025-03-11T20:20:42.221710Z"
    }
   },
   "cell_type": "code",
   "source": "print(macro_score_OHE_GB)",
   "id": "b7d495a09653bb57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8202423408128906\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T21:07:14.167990Z",
     "start_time": "2025-03-11T21:06:50.599939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clf_OHE_ADA = AdaBoostClassifier(estimator=RandomForestClassifier(n_estimators=12, max_depth=25, n_jobs=-1), n_estimators=50, learning_rate=0.3)\n",
    "clf_OHE_ADA = clf_OHE_ADA.fit(x_train_OHE, y_train)"
   ],
   "id": "a2e8169ffbf98d2e",
   "outputs": [],
   "execution_count": 174
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T21:07:15.924096Z",
     "start_time": "2025-03-11T21:07:15.163345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictions_ADA_OHE = clf_OHE_ADA.predict(x_test_OHE)\n",
    "macro_score_ADA_OHE = f1_score(y_test, predictions_ADA_OHE, average='macro')"
   ],
   "id": "3052665e5de5ca69",
   "outputs": [],
   "execution_count": 175
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T21:07:16.949890Z",
     "start_time": "2025-03-11T21:07:16.946520Z"
    }
   },
   "cell_type": "code",
   "source": "print(macro_score_ADA_OHE)",
   "id": "6fc30eaa5048f6e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8810723137667106\n"
     ]
    }
   ],
   "execution_count": 176
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T21:06:47.888494Z",
     "start_time": "2025-03-11T21:06:10.826511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clf_COUNT_ADA = AdaBoostClassifier(estimator=RandomForestClassifier(n_estimators=12, max_depth=25, n_jobs=-1), n_estimators=50, learning_rate=0.3)\n",
    "clf_COUNT_ADA = clf_OHE_ADA.fit(x_train_COUNT, y_train)"
   ],
   "id": "1438c82e8b12b1c7",
   "outputs": [],
   "execution_count": 171
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T21:06:48.611573Z",
     "start_time": "2025-03-11T21:06:47.894478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictions_ADA_COUNT = clf_COUNT_ADA.predict(x_test_COUNT)\n",
    "macro_score_ADA_COUNT = f1_score(y_test, predictions_ADA_COUNT, average='macro')"
   ],
   "id": "8f30fb3a2c979b39",
   "outputs": [],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T21:06:49.603917Z",
     "start_time": "2025-03-11T21:06:49.600539Z"
    }
   },
   "cell_type": "code",
   "source": "print(macro_score_ADA_COUNT)",
   "id": "b78ba8acc1d53066",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8812967726235373\n"
     ]
    }
   ],
   "execution_count": 173
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T21:04:32.899913Z",
     "start_time": "2025-03-11T21:04:04.594238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clf_TFIDF_ADA = AdaBoostClassifier(estimator=RandomForestClassifier(n_estimators=12, max_depth=25, n_jobs=-1), n_estimators=50, learning_rate=0.3)\n",
    "clf_TFIDF_ADA = clf_OHE_ADA.fit(x_train_TFIDF, y_train)"
   ],
   "id": "507c6b3ac751befb",
   "outputs": [],
   "execution_count": 168
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T21:04:33.609972Z",
     "start_time": "2025-03-11T21:04:32.902912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictions_ADA_TFIDF = clf_TFIDF_ADA.predict(x_test_TFIDF)\n",
    "macro_score_ADA_TFIDF = f1_score(y_test, predictions_ADA_TFIDF, average='macro')"
   ],
   "id": "f9f7824a85620f22",
   "outputs": [],
   "execution_count": 169
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T21:04:34.601829Z",
     "start_time": "2025-03-11T21:04:34.598830Z"
    }
   },
   "cell_type": "code",
   "source": "print(macro_score_ADA_TFIDF)",
   "id": "97848e42a96d9c33",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8857382437564187\n"
     ]
    }
   ],
   "execution_count": 170
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T21:07:51.634716Z",
     "start_time": "2025-03-11T21:07:51.630605Z"
    }
   },
   "cell_type": "code",
   "source": "clf_OHE_ADA.__class__.__name__",
   "id": "3069a355368ee407",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AdaBoostClassifier'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 177
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "18bac159e0b3f168"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
